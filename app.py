import streamlit as st
import os
from dotenv import load_dotenv

# 1. IMPORTS (The "Guest List")
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage

# Define our preferred working model here
WORKING_MODEL_NAME = "gemini-2.5-flash-lite" # Or whatever name you confirmed works!

# 2. SETUP SECRETS & DATA
load_dotenv()
user_phone = os.getenv("USER_PHONE")
user_email = os.getenv("USER_EMAIL")
contact_info = f"Phone: {user_phone}\nEmail: {user_email}"

# Load the CV text file immediately so the whole script can see it
if os.path.exists("my_cv.txt"):
    with open("my_cv.txt", "r") as file:
        content = file.read()
else:
    st.error("Missing my_cv.txt! Please create it in your project folder.")
    st.stop()

# Prepare the chunks (the "pages" for our librarian)
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = text_splitter.split_text(content)

# 3. SET UP THE ENGINE (Cached for performance)
@st.cache_resource
def init_rag():
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    # We use the stable 2.5-flash to avoid the "Limit 0" errors
    model = ChatGoogleGenerativeAI(model=WORKING_MODEL_NAME)
    
    persist_dir = "my_cv_database"
    
    # Check if we need to build a new database or load an old one
    if os.path.exists(persist_dir):
        vectorstore = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
    else:
        vectorstore = Chroma.from_texts(texts=chunks, embedding=embeddings, persist_directory=persist_dir)
    
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    return model, retriever

model, retriever = init_rag()

# 4. CONTEXT COMBINER
def get_full_context(input_data):
    actual_question = input_data["question"]
    docs = retriever.invoke(actual_question)
    cv_text = "\n\n".join(doc.page_content for doc in docs)
    return f"{cv_text}\n\nPRIVATE CONTACT INFO:\n{contact_info}"

# 5. UI LAYOUT
st.set_page_config(page_title="Asar's AI CV", page_icon="üöÄ")
st.title("ü§ñ Chat with Asar's Personal AI")

# Initialize Chat History in Session State
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Display previous messages
for message in st.session_state.chat_history:
    if isinstance(message, HumanMessage):
        with st.chat_message("Human"):
            st.markdown(message.content)
    else:
        with st.chat_message("AI"):
            st.markdown(message.content)

# 6. THE CONVEYOR BELT (Chain)
template = """
You are the personal AI Brand Ambassador for Asar Aygul. 

STRICT RULES:
1.  *FACTS:* For ALL factual questions (Education, Experience, Skills, Contact Info), you MUST ONLY use the 'Context from CV' provided below. Do not add anything that is not explicitly there.
2.  *NO FACTS FROM MEMORY:* Never use your general knowledge to state a fact about Asar.
3.  *OPINIONS & POLISH ARE YOUR JOB:* You MAY use your general knowledge ONLY to provide positive, professional evaluations (e.g., "Asar's Master's in Mine Engineering demonstrates a strong technical foundation...").
4.  *PERSONA:* Always speak in the first person ("I," "my," "I have").

Chat History:
{chat_history}

Context from CV:
{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

chain = (
    {
        "context": get_full_context, 
        "question": lambda x: x["question"],
        "chat_history": lambda x: x["chat_history"]
    }
    | prompt | model | StrOutputParser()
)

# 7. CHAT INPUT & EXECUTION
user_query = st.chat_input("Ask me about Asar's experience...")

if user_query:
    # Show user message immediately
    with st.chat_message("Human"):
        st.markdown(user_query)
    
    # Run the Chain
    response = chain.invoke({
        "question": user_query, 
        "chat_history": st.session_state.chat_history
    })
    
    # Show AI response
    with st.chat_message("AI"):
        st.markdown(response)
    
    # Add to history
    st.session_state.chat_history.append(HumanMessage(content=user_query))
    st.session_state.chat_history.append(AIMessage(content=response))

# 8. THE SPY HOLE (Debug Mode)
with st.sidebar:
    st.divider()
    with st.expander("üïµÔ∏è Debug: Librarian's Search Results"):
        if user_query:
            debug_info = get_full_context({"question": user_query})
            st.write(debug_info)
        else:
            st.write("Waiting for a question...")